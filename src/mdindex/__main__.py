"""
Create index files and table of contents for collections of markdown files to
make them easier to navigate once rendered in a browser.

Index files (named <INDEX_FILE>) will be processed to include a nested list of
sibling markdown files and subdirectories (up to <MAX_DEPTH>). If the file
doesn't exist it will be created with some basic structure, otherwise this looks
for a section delimited by `<!-- mdindex:index:start -->` and `<!--
mdindex:index:start -->` to replace.

Optionally the --toc flag will enable table of content generation within non
index files. The table of content will be inserted either between a section
delimited by `<!-- mdindex:toc:start -->` and `<!-- mdindex:toc:start -->`,
directly after the first H1 heading or directly before the first H2 heading.

--check can be used for dry runs and validating files have been processed.
"""

from __future__ import annotations

import argparse
import fnmatch
import logging
import re
import sys
import unicodedata
import urllib.parse
from dataclasses import dataclass, field
from pathlib import Path
from typing import TYPE_CHECKING, Literal

if TYPE_CHECKING:
    from collections.abc import Iterable, Sequence


__version__ = "0.1.0"

WARNING_FILE = "<!-- WARN: This file is auto-generated. Do not edit manually. -->"
WARNING_COMMAND = "<!-- command: {command} -->"
WARNING_SECTION = "<!-- WARN: This section is auto-generated. Do not edit manually. -->"

INDEX_START_MARKER = "<!-- mdindex:index:start -->"
INDEX_END_MARKER = "<!-- mdindex:index:end -->"

TOC_START_MARKER = "<!-- mdindex:toc:start -->"
TOC_END_MARKER = "<!-- mdindex:toc:end -->"


logger = logging.getLogger()


class ExitError(Exception):
    def __init__(self, source: str | Exception, filepath: Path | None = None) -> None:
        super().__init__(str(source) if not filepath else f"{filepath}: {source}")


class MutlipleMarkersError(ValueError):
    def __init__(self, marker: str) -> None:
        super().__init__(f"Multiple {marker} markers found")


class ReversedMarkersError(ValueError):
    def __init__(self, from_: str, to: str) -> None:
        super().__init__(f"{to} should be after {from_}")


class SkipIndexFileError(Exception):
    def __init__(self, path: Path) -> None:
        super().__init__(f"{path} already exists and is not generated by this command")


type Action = Literal["create", "recreate", "update"]


# Lazy bag of options for less verbose passing around
@dataclass(frozen=True, slots=True)
class Context:
    index_file: str = "README.md"
    command: str | None = None
    max_depth: int = 2
    render_toc: bool = False
    toc_min_length: int = 2
    toc_max_level: int = 3


@dataclass(frozen=True, slots=True)
class IndexNode:
    path: Path
    files: list[Path] = field(default_factory=list)
    children: list[IndexNode] = field(default_factory=list)


@dataclass(frozen=True, slots=True)
class Operation:
    action: Action
    path: Path
    lines: list[str]


@dataclass(frozen=True, slots=True)
class Header:
    level: int
    value: str
    underlined: bool
    lineno: int

    @property
    def previous_line(self) -> int:
        return self.lineno - 1

    @property
    def next_line(self) -> int:
        return self.lineno + 2 if self.underlined else self.lineno + 1


@dataclass(frozen=True, slots=True)
class HeaderNode:
    header: Header
    children: list[HeaderNode] = field(default_factory=list)


def filter_files(directory: Path, *, ignored_globs: Sequence[str] = ()) -> Iterable[Path]:
    for x in directory.glob("**/*.md"):
        if any(fnmatch.fnmatch(str(x), pattern) for pattern in ignored_globs):
            continue
        yield x


def build_tree(directory: Path, index_file: str, files: Iterable[Path]) -> IndexNode:
    """Collect all files to consider in a basic tree structure."""
    root = IndexNode(directory)

    for x in files:
        if x.name == index_file:  # Don't include index files at this stage.
            continue

        section = root
        *parents, _ = x.relative_to(directory).parts

        for parent in parents:
            if not (child := next((x for x in section.children if x.path.name == parent), None)):
                child = IndexNode(section.path / parent)
                section.children.append(child)
            section = child

        section.files.append(x)

    return root


def get_lines(filepath: Path) -> list[str]:
    return [x.rstrip("\n") for x in filepath.open().readlines()]


def titlecase(value: str) -> str:
    return re.sub(r"(?<=[a-z])(?=[A-Z])|[-_]", " ", value).title()


def get_title(filepath: Path) -> str:
    if header := next(iter(extract_headers(get_lines(filepath))), None):
        return header.value
    return titlecase(filepath.stem)


def get_section_title(path: Path, ctx: Context) -> str:
    index = path / ctx.index_file
    if index.exists() and (extracted := get_title(index)):
        return extracted
    return titlecase(path.name)


def find_marker_line(lines: list[str], marker: str) -> int | None:
    positions = [i for i, x in enumerate(lines) if marker in x]
    if not positions:
        return None

    if len(positions) > 1:
        raise MutlipleMarkersError(marker)

    return positions[0]


def find_marked_section(lines: list[str], from_: str, to: str) -> tuple[int, int] | None:
    start = find_marker_line(lines, from_)
    end = find_marker_line(lines, to)
    if start is None or end is None:
        return None
    if end <= start:
        raise ReversedMarkersError(from_, to)
    return start, end


def render_index(section: IndexNode, ctx: Context, *, max_depth: int) -> list[str]:
    directory = section.path

    def _write_section(section: IndexNode, level: int) -> Iterable[str]:
        if level > -1:
            quoted = urllib.parse.quote(str(section.path.relative_to(directory)))
            yield f"{('  ' * level)}- [{get_section_title(section.path, ctx)}](./{quoted})"

        if (level + 1) >= max_depth:
            return

        nested = level + 1

        for x in sorted(section.files):
            title = get_title(x)
            quoted = urllib.parse.quote(str(x.relative_to(directory)))
            yield f"{('  ' * nested)}- [{title}](./{quoted})"

        for y in sorted(section.children, key=lambda x: x.path.name):
            yield from _write_section(y, nested)

    return list(_write_section(section, -1))


def render_full_index(*, name: str, inner: list[str], command: str | None = None) -> list[str]:
    return [
        INDEX_START_MARKER,
        f"# {titlecase(name)}",
        "",
        WARNING_FILE,
        *([f"{WARNING_COMMAND.format(command=command)}"] if command else ()),
        "",
        *inner,
        INDEX_END_MARKER,
    ]


def render_marked_index(*, inner: list[str], command: str | None = None) -> list[str]:
    return [
        INDEX_START_MARKER,
        WARNING_SECTION,
        *([f"{WARNING_COMMAND.format(command=command)}"] if command else ()),
        "",
        *inner,
        "",
        INDEX_END_MARKER,
    ]


def insert_between(lines: list[str], start: int, end: int, inner: list[str]) -> list[str]:
    before = [x for i, x in enumerate(lines) if i < start]
    after = [x for i, x in enumerate(lines) if i > end]
    if before and before[-1].strip():
        inner = ["", *inner]
    if after and after[0].strip():
        inner = [*inner, ""]
    return [*before, *inner, *after]


def index_operations(section: IndexNode, ctx: Context) -> Operation:
    index = render_index(section, ctx, max_depth=ctx.max_depth)
    output_file = section.path / ctx.index_file

    if output_file.exists():
        lines = get_lines(output_file)
        try:
            markers = find_marked_section(lines, INDEX_START_MARKER, INDEX_END_MARKER)
        except (MutlipleMarkersError, ReversedMarkersError) as e:
            raise ExitError(e, output_file) from None
        if markers is None:
            raise SkipIndexFileError(output_file)
        else:
            start, end = markers
            if start == 0:
                contents = render_full_index(
                    name=section.path.name,
                    inner=index,
                    command=ctx.command,
                )
                return Operation("recreate", output_file, contents)
            else:
                contents = insert_between(
                    lines,
                    start,
                    end,
                    render_marked_index(inner=index, command=ctx.command),
                )
                return Operation("update", output_file, contents)
    else:
        contents = render_full_index(name=section.path.name, inner=index, command=ctx.command)
        return Operation("create", output_file, contents)


def extract_headers(lines: list[str]) -> Iterable[Header]:
    for i, (a, b) in enumerate(zip(lines, [*lines[1:], None])):
        if match := re.match(r"^(#+) (.*)$", a):
            level = len(match.group(1))
            yield Header(level, match.group(2), underlined=False, lineno=i)
        if b and re.match(r"==+", b) and (inner := a.strip()):
            yield Header(1, inner, underlined=True, lineno=i)
        if b and re.match(r"--+", b) and (inner := a.strip()):
            yield Header(2, inner, underlined=True, lineno=i)


def build_headers_tree(headers: Iterable[Header], min_level: int = 2) -> Iterable[HeaderNode]:
    root_nodes: list[HeaderNode] = []
    stack: list[HeaderNode] = []

    for header in headers:
        if header.level < min_level:
            continue

        node = HeaderNode(header=header)

        while stack and header.level <= stack[-1].header.level:
            stack.pop()

        if stack:
            stack[-1].children.append(node)
        else:
            root_nodes.append(node)

        stack.append(node)

    return root_nodes


def slugify(value: str) -> str:
    # This should mostly end up matching GFM's approach for most things
    # Adapted from Django's slugify util
    value = unicodedata.normalize("NFKC", value)
    value = re.sub(r"[^\w\s-]", "", value.lower())
    return re.sub(r"[-\s]+", "-", value).strip("-_")


def toc_lines(nodes: list[HeaderNode], *, start_depth: int = 0, max_depth: int) -> Iterable[str]:
    for x in nodes:
        yield f"{('  ' * start_depth)}- [{x.header.value}](#{slugify(x.header.value)})"
        if start_depth < max_depth:
            yield from toc_lines(x.children, start_depth=start_depth + 1, max_depth=max_depth)


def render_toc(inner: list[str], ctx: Context) -> list[str]:
    return [
        TOC_START_MARKER,
        WARNING_SECTION,
        *([f"{WARNING_COMMAND.format(command=ctx.command)}"] if ctx.command else ()),
        "",
        *inner,
        "",
        TOC_END_MARKER,
    ]


def guess_toc_location_and_level(headers: list[Header]) -> tuple[tuple[int, int], int]:
    min_level = min(x.level for x in headers)
    first = headers[0]
    at_min_level = [x for x in headers if x.level == min_level]

    if len(at_min_level) > 1 and first.level == min_level:
        # Multiple top levels as long as everything exists under one of them, we
        # put it before the first entry and include the top level.
        return (first.previous_line, first.previous_line), min_level
    elif len(at_min_level) == 1 and first.level == min_level:
        # Single top level as long as everything is under it, we put it after
        # the first entry and do not include the top level as it serves as
        # document title.
        return (first.next_line, first.next_line), min_level + 1
    else:
        # We may have a top level entry but some headers at lower levels come
        # before it just put it at the top and include all.
        return (0, 0), min_level


def toc_operation(filepath: Path, ctx: Context) -> Operation | None:
    lines = get_lines(filepath)
    headers = list(extract_headers(lines))

    if len(headers) < ctx.toc_min_length:
        return None

    section_marks = find_marked_section(lines, TOC_START_MARKER, TOC_END_MARKER)

    if section_marks is not None:
        # We know where to put the TOC, assume well formed and just ignore the top level.
        start, end = section_marks
        min_included_level = min(x.level for x in headers) + 1
    else:
        # Otherwise guesswork to find a rational place to put it
        (start, end), min_included_level = guess_toc_location_and_level(headers)

    toc = list(
        toc_lines(
            list(build_headers_tree(headers, min_level=min_included_level)),
            max_depth=ctx.toc_max_level,
        )
    )
    if len(toc) >= ctx.toc_min_length:
        inner = render_toc(toc, ctx)
        return Operation("update", filepath, insert_between(lines, start, end, inner))
    return None


def process(root: IndexNode, ctx: Context, *, recursive: bool = False) -> Iterable[Operation]:
    try:
        yield index_operations(root, ctx)
    except SkipIndexFileError as e:
        logger.debug(str(e))

    if ctx.render_toc:
        for fp in root.files:
            if op := toc_operation(fp, ctx):
                yield op

    if recursive:
        for x in root.children:
            yield from process(x, ctx, recursive=recursive)


def apply(op: Operation) -> None:
    contents = "\n".join(op.lines) + "\n"
    if (not op.path.exists()) or op.path.read_text() != contents:
        logger.debug("Writing to file %s (%s)", op.path, op.action)
        op.path.write_text(contents)


def dry_run(op: Operation) -> bool:
    contents = "\n".join(op.lines) + "\n"
    logger.info("Would write to file %s (%s)", op.path, op.action)
    if (not op.path.exists()) or op.path.read_text() != contents:
        logger.info("%s is not up to date", op.path)
        return True
    return False


def main() -> None:
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument("--version", action="version", version=f"%(prog)s {__version__}")
    parser.add_argument("--verbose", "-v", help="Print debug logs to stderr", action="store_true")

    parser.add_argument("directory", type=Path, default="docs", help="Root directory")

    parser.add_argument(
        "--index-file",
        type=str,
        default="README.md",
        required=False,
        help="Index file name",
    )

    parser.add_argument(
        "--command",
        type=str,
        required=False,
        help="Command used for generating the index files",
    )

    parser.add_argument(
        "--max-depth",
        type=int,
        default=3,
        required=False,
        help="Only list files in directories up to <MAX_DEPTH> from the current index file.",
    )

    parser.add_argument(
        "--toc",
        action="store_true",
        help="Enable generation of table of contents within non-index files.",
    )
    parser.add_argument(
        "--toc-min-length",
        type=int,
        default=2,
        help=(
            "Only generate table of contents within files for which it would "
            "end be longer tha <TOC_MIN_LENGTH> entries."
        ),
    )
    parser.add_argument(
        "--toc-max-level",
        choices=(2, 3, 4, 5, 6),
        type=int,
        default=3,
        help="Only generate table of contents for headers up to this level.",
    )

    parser.add_argument(
        "--recursive",
        "-r",
        action="store_true",
        help="Also process subdirectories recursively.",
    )
    parser.add_argument(
        "--ignore",
        "-i",
        type=str,
        action="append",
        default=[],
        required=False,
        help=(
            "Glob patterns to ignore (repeatable). This applies to input "
            "directory and files and relies on fnmatch behaviour on the path "
            "relative to the root directory."
        ),
    )

    parser.add_argument(
        "--check",
        "-c",
        action="store_true",
        help="Check if the index files are up to date.",
    )

    args = parser.parse_args()

    logger.addHandler(logging.StreamHandler(sys.stderr))
    logger.setLevel(logging.DEBUG if args.verbose else logging.INFO)

    ctx = Context(
        index_file=args.index_file,
        command=args.command,
        max_depth=args.max_depth,
        render_toc=args.toc,
        toc_min_length=args.toc_min_length,
        toc_max_level=args.toc_max_level,
    )

    directory = Path(args.directory)

    try:
        files = filter_files(directory, ignored_globs=tuple(args.ignore))
        root = build_tree(directory, ctx.index_file, files)
        operations = process(root, ctx, recursive=args.recursive)

        if args.check:
            if any(not dry_run(op) for op in operations):
                if ctx.command:
                    logger.info("To update run: %s", ctx.command)
                sys.exit(1)
        else:
            for op in operations:
                apply(op)

    except ExitError as e:
        logger.error(e)
        sys.exit(1)


if __name__ == "__main__":
    main()
